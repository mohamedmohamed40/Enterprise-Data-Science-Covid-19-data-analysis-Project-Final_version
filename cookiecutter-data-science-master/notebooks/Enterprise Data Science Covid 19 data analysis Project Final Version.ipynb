{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise Data Science: Covid 19 Data Analysis Project "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![CRISP_DM](../reports/figures/CRISP_DM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine Business Objectives\n",
    "The main goal is  tracking the  Corona VIRUS spread across countries and with personal local information\n",
    "\n",
    "The general knowledge to me is not that important\n",
    "\n",
    "I would like deep dive local development of the spreading \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Understanding  the data quality\n",
    "\n",
    "2-Automated  process as much as possible:\n",
    "    how many clicks do we need to execute the full pipeline\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contraints:\n",
    "\n",
    "* Each notebook should be left clean and ready for full execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up The Project\n",
    "Install Cookiecutter  for having clear structure for the project\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding (Data_gathering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering  the data from:\n",
    "\n",
    "1-RKI ;webscrape (webscraping)\n",
    "\n",
    "2-John Hopkins (GITHUB)\n",
    "\n",
    "3-REST API services to retreive data\n",
    "\n",
    "The data for John Hopkins is updating daily, the data is formatted in time series  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Import Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining all imports\n",
    "'''all imports should be at the beginning '''\n",
    "import subprocess                   \n",
    "import os\n",
    "\n",
    "import pandas as pd                 \n",
    "import numpy as np\n",
    "from scipy import signal                                            # to import the fliter pakage \n",
    "from datetime import datetime                                       # day time lib\n",
    "\n",
    "from sklearn import linear_model                                    # import Scikit Learn library\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)             # calling object reg .regression is initialize as object\n",
    "\n",
    "import dash                                                         # for server client communication\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output,State\n",
    "\n",
    "import plotly.graph_objects as go                                   # install plotly graph objects\n",
    "\n",
    "import matplotlib as mpl                                            # library for plot figure \n",
    "import matplotlib. pyplot as plt                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.Data Understanding Phase (Getting and Updating The Data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : b''\n",
      "out : b'Already up to date.\\n'\n"
     ]
    }
   ],
   "source": [
    "#defining the johns hopkins data and automated the gitHUB pull request\n",
    "\n",
    "def updating_johns_hopkins_data():                         \n",
    "             #the function description: getting the johns hopkins data and then updating it  automatically \n",
    "    ''' first selecting the johns hopkins data ,\n",
    "    second we will clone the data \n",
    "    then we get pull the data from johns jopkins \n",
    "    the result will be stored in the predifined csv structure\n",
    "    \n",
    "    '''\n",
    "    git_pull = subprocess.Popen( \"git pull\" ,        # the subproces LIb is used to access the terminal(using command clone for first time to get the data )\n",
    "                         cwd = os.path.dirname( '../data/raw/COVID-19/' ),  # defining the data path for the git pull command\n",
    "                         shell = True,\n",
    "                         stdout = subprocess.PIPE,\n",
    "                         stderr = subprocess.PIPE )              # getting back the pipeline standard out and standard error \n",
    "    (out, error) = git_pull.communicate()                        # to automated the GITHUB Request\n",
    "    \n",
    "    \n",
    "    print(\"Error : \" + str(error))\n",
    "    print(\"out : \" + str(out))                          \n",
    "    \n",
    "updating_johns_hopkins_data()        #call the function ,once run this cell the get pull is done in automatice way "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.Checking The Data Format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data  is stored  in predefined folder,getting the data and show it's format.\n",
    "we can look at data set by typing in the data frame \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province/State</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>1/22/20</th>\n",
       "      <th>1/23/20</th>\n",
       "      <th>1/24/20</th>\n",
       "      <th>1/25/20</th>\n",
       "      <th>1/26/20</th>\n",
       "      <th>1/27/20</th>\n",
       "      <th>...</th>\n",
       "      <th>8/19/20</th>\n",
       "      <th>8/20/20</th>\n",
       "      <th>8/21/20</th>\n",
       "      <th>8/22/20</th>\n",
       "      <th>8/23/20</th>\n",
       "      <th>8/24/20</th>\n",
       "      <th>8/25/20</th>\n",
       "      <th>8/26/20</th>\n",
       "      <th>8/27/20</th>\n",
       "      <th>8/28/20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>33.93911</td>\n",
       "      <td>67.709953</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>37599</td>\n",
       "      <td>37856</td>\n",
       "      <td>37894</td>\n",
       "      <td>37953</td>\n",
       "      <td>37999</td>\n",
       "      <td>38054</td>\n",
       "      <td>38070</td>\n",
       "      <td>38113</td>\n",
       "      <td>38129</td>\n",
       "      <td>38140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Albania</td>\n",
       "      <td>41.15330</td>\n",
       "      <td>20.168300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7812</td>\n",
       "      <td>7967</td>\n",
       "      <td>8119</td>\n",
       "      <td>8275</td>\n",
       "      <td>8427</td>\n",
       "      <td>8605</td>\n",
       "      <td>8759</td>\n",
       "      <td>8927</td>\n",
       "      <td>9083</td>\n",
       "      <td>9195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>28.03390</td>\n",
       "      <td>1.659600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39847</td>\n",
       "      <td>40258</td>\n",
       "      <td>40667</td>\n",
       "      <td>41068</td>\n",
       "      <td>41460</td>\n",
       "      <td>41858</td>\n",
       "      <td>42228</td>\n",
       "      <td>42619</td>\n",
       "      <td>43016</td>\n",
       "      <td>43403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>42.50630</td>\n",
       "      <td>1.521800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1024</td>\n",
       "      <td>1024</td>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "      <td>1045</td>\n",
       "      <td>1060</td>\n",
       "      <td>1060</td>\n",
       "      <td>1098</td>\n",
       "      <td>1098</td>\n",
       "      <td>1124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Angola</td>\n",
       "      <td>-11.20270</td>\n",
       "      <td>17.873900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2015</td>\n",
       "      <td>2044</td>\n",
       "      <td>2068</td>\n",
       "      <td>2134</td>\n",
       "      <td>2171</td>\n",
       "      <td>2222</td>\n",
       "      <td>2283</td>\n",
       "      <td>2332</td>\n",
       "      <td>2415</td>\n",
       "      <td>2471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 224 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Province/State Country/Region       Lat       Long  1/22/20  1/23/20  \\\n",
       "0            NaN    Afghanistan  33.93911  67.709953        0        0   \n",
       "1            NaN        Albania  41.15330  20.168300        0        0   \n",
       "2            NaN        Algeria  28.03390   1.659600        0        0   \n",
       "3            NaN        Andorra  42.50630   1.521800        0        0   \n",
       "4            NaN         Angola -11.20270  17.873900        0        0   \n",
       "\n",
       "   1/24/20  1/25/20  1/26/20  1/27/20  ...  8/19/20  8/20/20  8/21/20  \\\n",
       "0        0        0        0        0  ...    37599    37856    37894   \n",
       "1        0        0        0        0  ...     7812     7967     8119   \n",
       "2        0        0        0        0  ...    39847    40258    40667   \n",
       "3        0        0        0        0  ...     1024     1024     1045   \n",
       "4        0        0        0        0  ...     2015     2044     2068   \n",
       "\n",
       "   8/22/20  8/23/20  8/24/20  8/25/20  8/26/20  8/27/20  8/28/20  \n",
       "0    37953    37999    38054    38070    38113    38129    38140  \n",
       "1     8275     8427     8605     8759     8927     9083     9195  \n",
       "2    41068    41460    41858    42228    42619    43016    43403  \n",
       "3     1045     1045     1060     1060     1098     1098     1124  \n",
       "4     2134     2171     2222     2283     2332     2415     2471  \n",
       "\n",
       "[5 rows x 224 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path=r'../data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "pd_raw=pd.read_csv(data_path)   \n",
    "pd_raw.head()    # extra info "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the final data structure,the data  is stored  in predefined folder.\n",
    "\n",
    "Data preparation phase will focus on johns hopkins data set , time series format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1. Relational Johns Hopkins Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 58520\n",
      " Latest date is: 2020-08-28 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def Relational_data_set_JH():\n",
    "    ''' reconstruction  the COVID data timeseries in a relational data set\n",
    "\n",
    "    '''\n",
    "          # defining a new data structure \n",
    "    data_path=r'../data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "                                    # to define the data path , using r'.. for windows system \n",
    "    pd_raw=pd.read_csv(data_path)   # to import the data  and read it ,in time series format\n",
    "\n",
    "    pd_data_base=pd_raw.rename(columns={'Country/Region':'country',\n",
    "                      'Province/State':'state'})    # rename the country/region/state                      \n",
    "\n",
    "    pd_data_base['state']=pd_data_base['state'].fillna('no')  # to avoid error with NAN valuesin state,some countries doesnt incloud states therefore we write no in that case \n",
    "\n",
    "    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1) # drop the lat and long axis from the data\n",
    "                                                          #axis=1 when dropping columns & axis=0 n dropping rows\n",
    "\n",
    "# T. transpose the matrix  in raw level , country in coloum level\n",
    "#stack.to bring multi index from columns to Raw base 0 for singel level \n",
    "#then name level 0 with data and the confirmed column also \n",
    "    pd_relational_final_data_model=pd_data_base.set_index(['state','country']) \\\n",
    "                                .T                              \\\n",
    "                                .stack(level=[0,1])             \\\n",
    "                                .reset_index()                  \\\n",
    "                                .rename(columns={'level_0':'date',\n",
    "                                                   0:'confirmed'},\n",
    "                                                  )\n",
    "#the type of data was string object , we should convert it to date object with using datetime64 (day time lib)\n",
    "\n",
    "    pd_relational_final_data_model['date']=pd_relational_final_data_model.date.astype('datetime64[ns]')\n",
    "\n",
    "    pd_relational_final_data_model.to_csv('../data/processed/COVID_relational_confirmed.csv',sep=';',index=False) #store data model to processed\n",
    "    print(' Number of rows stored: '+str(pd_relational_final_data_model.shape[0]))\n",
    "    print(' Latest date is: '+str(max(pd_relational_final_data_model.date)))\n",
    "# number of rows stored >> we process the john data and we stored the 54530  rows \n",
    "Relational_data_set_JH()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2.Checking The Data Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>confirmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Canada</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Anguilla</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Anhui</td>\n",
       "      <td>China</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Aruba</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Australian Capital Territory</td>\n",
       "      <td>Australia</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date                         state         country  confirmed\n",
       "0  2020-01-22                       Alberta          Canada        0.0\n",
       "1  2020-01-22                      Anguilla  United Kingdom        0.0\n",
       "2  2020-01-22                         Anhui           China        1.0\n",
       "3  2020-01-22                         Aruba     Netherlands        0.0\n",
       "4  2020-01-22  Australian Capital Territory       Australia        0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path=r'../data/processed/COVID_relational_confirmed.csv'\n",
    "pd_plot=pd.read_csv(data_path,sep=';')   # to look at the  data set after Relational\n",
    "pd_plot.head()    # extra info "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Regression Calculation and Full Process Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n",
      "            date state  country  confirmed  confirmed_filtered  confirmed_DR  \\\n",
      "31895 2020-08-24    no  Germany   236122.0            236214.0    207.718414   \n",
      "31896 2020-08-25    no  Germany   237583.0            237556.0    152.843207   \n",
      "31897 2020-08-26    no  Germany   239010.0            239082.4    164.523315   \n",
      "31898 2020-08-27    no  Germany   240571.0            240582.0    160.009817   \n",
      "31899 2020-08-28    no  Germany   242126.0            242081.6    154.408858   \n",
      "\n",
      "       confirmed_filtered_DR  \n",
      "31895             191.490493  \n",
      "31896             186.165472  \n",
      "31897             165.679450  \n",
      "31898             158.012866  \n",
      "31899             160.430782  \n"
     ]
    }
   ],
   "source": [
    "def Doubling_time_calc_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        slope\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "    #preparation our data for regression \n",
    "    y = np.array(in_array)                               # target vector\n",
    "    X = np.arange(-1,2).reshape(-1, 1)                   # input vector#columns represtent features, raw represent samples\n",
    "\n",
    "    assert len(in_array)==3                              \n",
    "    reg.fit(X,y)                                         # prediction and training the model with fit  \n",
    "    intercept=reg.intercept_                             # adjust the interception point \n",
    "    slope=reg.coef_                                      # coefficient , paramtere for slop \n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input,column='confirmed',window=5): #plug in filter\n",
    "    ''' Savgol Filter which can be used in groupby apply function (data structure kept)\n",
    "\n",
    "        parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "# to enusre that we will not destroyed index , the inputs data frame is stored  as result ( df_result=df_input)\n",
    "# adding new column     (df_result[str(column+'_filtered')]=result)\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0)                     # attention with the neutral element \n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),         # calling _saving the filtered data  in new data column\n",
    "                           5, # window size used for filtering for smoothing the data\n",
    "                           1) # polynomial degree ,ploynomial 1 is linear  \n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "\n",
    "def robe_reg(df_input,col='confirmed'):    #write a robe a round function doubling function (rolling function ),\n",
    "    #rolling move the windows a cross time series \n",
    "    ''' Rolling Regression to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame getting dataframe as input\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(                      #slice out the column ,doing the rolling days back ,rolling command move the windows cross time series \n",
    "                window=days_back,                      #freezing as a parameter days back \n",
    "                min_periods=days_back).apply(Doubling_time_calc_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "#getting the inputs , calc filter data and merge new features and new calcauating \n",
    "results as new column then passing back the large file data\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['state','country',filter_on])       #cross check that everything is ok (assert command)\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['state','country',filter_on]].groupby(['state','country']).apply(savgol_filter)\n",
    "\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='confirmed'):          #current slope ,trend line of doubling the data\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "#the same as calc_filtered_data function ,first  check that the column all in data set \n",
    "    must_contain=set(['state','country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "# applying groupby function on robe rolling reg filter\n",
    "#in that case we rename it before merge it back in our input data frame to get our output\n",
    "    pd_DR_result= df_input.groupby(['state','country']).apply(robe_reg,filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR',\n",
    "                             'level_2':'index'})\n",
    "\n",
    "    #we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "\n",
    "\n",
    "    return df_output\n",
    "\n",
    "# full script to process data set\n",
    "\n",
    "    #REG   object regression( classical Linear Regression)\n",
    "\n",
    "def final_data_calc():         # small test structure , it mean we get the test data  with array 2 4 6 and push it in our get_doubling_time_via_regression then we can executed \n",
    "    test_data_reg=np.array([2,4,6])\n",
    "    result=Doubling_time_calc_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    pd_JH_data=pd.read_csv(r'../data/processed/COVID_relational_confirmed.csv',sep=';',parse_dates=[0]) # reading the large data set \n",
    "    pd_JH_data=pd_JH_data.sort_values('date',ascending=True).copy()  # sorting the value is important \n",
    "\n",
    "    pd_result_larg=calc_filtered_data(pd_JH_data)   # first calc the filter data , we get back new confirmed filter data set\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg) # second calc doubling rate on default parameters \n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg,'confirmed_filtered') # final calc doubling rate on confirmed filter \n",
    "\n",
    "\n",
    "    mask=pd_result_larg['confirmed']>100  # to clean up doubling rate fiter (avoid NAN values), defining mask vector(0 1 vector) whatever the condition if fullfilled is true otherwise it will be false  \n",
    "    pd_result_larg['confirmed_filtered_DR']=pd_result_larg['confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv('../data/processed/COVID_final_set.csv',sep=';',index=False)\n",
    "    print(pd_result_larg[pd_result_larg['country']=='Germany'].tail())\n",
    "final_data_calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " Warning: This is a development server. Do not use app.run_server\n",
      " in production, use a production WSGI server like gunicorn instead.\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# creating dashboard and definig its layout and markdown for it. Adding traces and defining  the dictionary \n",
    "\n",
    "df_input_large=pd.read_csv(r'../data/processed/COVID_final_set.csv',sep=';')\n",
    "\n",
    "#generate figure object \n",
    "#with that object we can add different traces lines chart styles\n",
    "\n",
    "fig = go.Figure()      \n",
    "# starting defining the layout \n",
    "app = dash.Dash()\n",
    "app.layout = html.Div([\n",
    "\n",
    "    #Defining the header in dashboard\n",
    "    dcc.Markdown('''\n",
    "    Applied Data Science on COVID-19 data\n",
    "    \n",
    "    The project Goal:\n",
    "    applying a cross industry standard process to teach data science \n",
    "    covering the full walkthrough of : \n",
    "                    automated data gathering \n",
    "                    data transformations\n",
    "                    filtering and machine learning to approximating the doubling time\n",
    "                    static) deployment of responsive dashboard\n",
    "    '''),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    ## Multi-Select Country for visualization\n",
    "    '''),\n",
    "\n",
    "#first dropdown option to select the individual countries \n",
    "    #predefine each of possible option in hard coded way,construct our list of dictionaries in one nested loop\n",
    "    dcc.Dropdown(\n",
    "        id='country_drop_down',\n",
    "        options=[ {'label': each,'value':each} for each in df_input_large['country'].unique()],\n",
    "        value=['US', 'Germany','Italy','Egypt'], # which are pre-selected\n",
    "        multi=True    #many of countries can be selceted in our list\n",
    "    ),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "        ## Select Timeline of confirmed COVID-19 cases or the approximated doubling time\n",
    "        '''),\n",
    "    \n",
    "#second dropdown list for selecting between the data type(confirmed,confirmed_filtered,confirmed_DR,confirmed_filtered_DR)\n",
    " # in that dropdown only one is allowed to be choosen  <multi=False>  \n",
    "    dcc.Dropdown(\n",
    "    id='doubling_time',\n",
    "    options=[\n",
    "        {'label': 'Timeline Confirmed ', 'value': 'confirmed'},\n",
    "        {'label': 'Timeline Confirmed Filtered', 'value': 'confirmed_filtered'},\n",
    "        {'label': 'Timeline Doubling Rate', 'value': 'confirmed_DR'},\n",
    "        {'label': 'Timeline Doubling Rate Filtered', 'value': 'confirmed_filtered_DR'},\n",
    "    ],\n",
    "    value='confirmed',\n",
    "    multi=False  # in that dropdown only one is allowed to be choosen\n",
    "    ),\n",
    "\n",
    "    dcc.Graph(figure=fig, id='main_window_slope')   # the figure will be down the dropdown list , writting the command up will creat the figure above\n",
    "])\n",
    "\n",
    "#ending defining the layout \n",
    "\n",
    "# input and output countrol option \n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('country_drop_down', 'value'),\n",
    "    Input('doubling_time', 'value')])\n",
    "def update_figure(country_list,show_doubling):\n",
    "\n",
    "#Adiing  If function to display the text 'approximated doubling..... on x axis' if we choose the confirmed_filtered_DR',else it will show 'Confirmed infected people' text\n",
    "    if 'confirmed_filtered_DR' in show_doubling:\n",
    "        my_yaxis={'type':\"log\",\n",
    "               'title':'Approximated doubling rate over 3 days (larger numbers are better #stayathome)' # deinfe y axis in first selecting\n",
    "              }\n",
    "    else:\n",
    "        my_yaxis={'type':\"log\",\n",
    "                  'title':'Confirmed infected people (source johns hopkins csse, log-scale)' #define y axis in second selecting\n",
    "              }\n",
    "\n",
    "\n",
    "    traces = []    #adding traces and defining  the dictionary for it\n",
    "    for each in country_list:                  #bring in all other countries in the figure therefor we loop it(looping over the individual countries)\n",
    "\n",
    "        df_plot=df_input_large[df_input_large['country']==each]  \n",
    "\n",
    "        if show_doubling=='doubling_rate_filtered':\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot=df_plot[['state','country','confirmed','confirmed_filtered','confirmed_DR','confirmed_filtered_DR','date']].groupby(['country','date']).agg(np.sum).reset_index()\n",
    "\n",
    "            \n",
    "#adding traces and defining the dictionary for it ,and  for x axis we go for date and y axis we go show doubling results  \n",
    "#defines how to plot the trace \n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                                y=df_plot[show_doubling],\n",
    "                                mode='markers+lines',      # change the individual traces (markers,lines,....)\n",
    "                                opacity=1,                 #the color intense\n",
    "                                name=each\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    return {                           # retunr a dictionary with all data and traces \n",
    "            'data': traces,             # that part for defining the overall layout \n",
    "            'layout': dict (            #visualized the layout (size), define the layout properties \n",
    "                width=1280,\n",
    "                height=720,\n",
    "\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "\n",
    "                yaxis=my_yaxis\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "app.run_server(debug=True, use_reloader=False)     #launching the server \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
